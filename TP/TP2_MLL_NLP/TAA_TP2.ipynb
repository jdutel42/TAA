{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : Apprentissage multi-label sur des données textuelles\n",
    "\n",
    "*Jordan Dutel et Ariane Paradan*\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import function_TAA as taa\n",
    "importlib.reload(taa)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = taa.load_data('PubMed-multi-label-dataset.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.get_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Text cleaning (lowercase, tokenize, stop word...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction est longue à faire tourner (~4 min)\n",
    "\n",
    "moi ça m'a pris 13.5s (Ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = taa.clean_text(df, text_column='abstractText')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Features and labels split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']\n",
    "X, y = taa.split_feature_label(df, target_column, multilabel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = taa.split_train_test(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_matrix, X_test_tfidf_matrix, feature_names = taa.tfidf_vectorize2(X_train, X_test, max_features=2000, text_column_name='abstractText')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Sans extraction des concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre 10-30-50min pour run le ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = taa.run_models(X_train_tfidf_matrix, X_test_tfidf_matrix, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running MultiOutputClassifier...\n",
    "Evaluating MultiOutputClassifier...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.73      0.76     11641\n",
    "           1       0.95      0.99      0.97     23274\n",
    "           2       0.86      0.83      0.84     13179\n",
    "           3       0.90      0.89      0.89     15486\n",
    "           4       0.81      0.96      0.88     19629\n",
    "           5       0.84      0.58      0.69      4437\n",
    "           6       0.82      0.88      0.85     16802\n",
    "           7       0.58      0.09      0.15      3048\n",
    "           8       0.71      0.40      0.51      2839\n",
    "           9       0.72      0.23      0.35      2723\n",
    "          10       0.73      0.32      0.45      3729\n",
    "          11       0.86      0.87      0.86     10623\n",
    "          12       0.81      0.76      0.78     11447\n",
    "          13       0.74      0.50      0.60      4024\n",
    "\n",
    "   micro avg       0.85      0.81      0.83    142881\n",
    "   macro avg       0.79      0.64      0.68    142881\n",
    "weighted avg       0.84      0.81      0.81    142881\n",
    " samples avg       0.85      0.82      0.82    142881\n",
    "\n",
    "\n",
    "Running ClassifierChain...\n",
    "Evaluating ClassifierChain...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.68      0.71      0.70     11641\n",
    "           1       0.94      0.99      0.96     23274\n",
    "           2       0.77      0.78      0.77     13179\n",
    "           3       0.83      0.85      0.84     15486\n",
    "           4       0.81      0.92      0.86     19629\n",
    "           5       0.71      0.49      0.58      4437\n",
    "           6       0.79      0.86      0.82     16802\n",
    "           7       0.49      0.10      0.16      3048\n",
    "           8       0.60      0.38      0.46      2839\n",
    "           9       0.58      0.17      0.26      2723\n",
    "          10       0.56      0.25      0.35      3729\n",
    "          11       0.76      0.81      0.78     10623\n",
    "          12       0.71      0.71      0.71     11447\n",
    "          13       0.56      0.45      0.50      4024\n",
    "\n",
    "   micro avg       0.79      0.77      0.78    142881\n",
    "   macro avg       0.70      0.60      0.62    142881\n",
    "weighted avg       0.77      0.77      0.76    142881\n",
    " samples avg       0.79      0.78      0.77    142881\n",
    "\n",
    "\n",
    "Results Summary:\n",
    "MultiOutputClassifier Metrics: {'micro-F1': 0.8283539043046773, 'macro-F1': 0.6839475404492766, 'zero-one-loss': 0.86248}\n",
    "ClassifierChain Metrics: {'micro-F1': 0.7802059961486141, 'macro-F1': 0.6248012402638302, 'zero-one-loss': 0.91164}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Avec extraction des concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd, X_train_svd_matrix, X_test_svd_matrix = taa.extract_concept_SVD(2, X_train_tfidf_matrix, X_test_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les 10 mots les plus pertinents pour chaque concept\n",
    "taa.print_top_words(svd, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = taa.run_models(X_train_svd_matrix, X_test_svd_matrix, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running MultiOutputClassifier...\n",
    "Evaluating MultiOutputClassifier...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.68      0.52      0.59     11641\n",
    "           1       0.93      1.00      0.96     23274\n",
    "           2       0.76      0.68      0.72     13179\n",
    "           3       0.79      0.82      0.80     15486\n",
    "           4       0.79      1.00      0.88     19629\n",
    "           5       0.00      0.00      0.00      4437\n",
    "           6       0.74      0.90      0.81     16802\n",
    "           7       0.00      0.00      0.00      3048\n",
    "           8       0.00      0.00      0.00      2839\n",
    "           9       0.00      0.00      0.00      2723\n",
    "          10       0.00      0.00      0.00      3729\n",
    "          11       0.81      0.75      0.78     10623\n",
    "          12       0.73      0.67      0.70     11447\n",
    "          13       0.25      0.00      0.00      4024\n",
    "\n",
    "   micro avg       0.79      0.71      0.75    142881\n",
    "   macro avg       0.46      0.45      0.45    142881\n",
    "weighted avg       0.68      0.71      0.69    142881\n",
    " samples avg       0.79      0.72      0.74    142881\n",
    "\n",
    "\n",
    "Running ClassifierChain...\n",
    "Evaluating ClassifierChain...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.57      0.61      0.59     11641\n",
    "           1       0.94      0.98      0.96     23274\n",
    "           2       0.72      0.74      0.73     13179\n",
    "           3       0.78      0.82      0.80     15486\n",
    "           4       0.79      0.92      0.85     19629\n",
    "           5       0.33      0.11      0.16      4437\n",
    "           6       0.75      0.83      0.79     16802\n",
    "           7       0.17      0.01      0.02      3048\n",
    "           8       0.27      0.07      0.11      2839\n",
    "           9       0.20      0.04      0.06      2723\n",
    "          10       0.21      0.04      0.07      3729\n",
    "          11       0.77      0.81      0.79     10623\n",
    "          12       0.69      0.64      0.67     11447\n",
    "          13       0.32      0.12      0.17      4024\n",
    "\n",
    "   micro avg       0.75      0.71      0.73    142881\n",
    "   macro avg       0.54      0.48      0.48    142881\n",
    "weighted avg       0.70      0.71      0.70    142881\n",
    " samples avg       0.76      0.72      0.72    142881\n",
    "\n",
    "\n",
    "Results Summary:\n",
    "MultiOutputClassifier Metrics: {'micro-F1': 0.7492805183961933, 'macro-F1': 0.44633911518524305, 'zero-one-loss': 0.93528}\n",
    "ClassifierChain Metrics: {'micro-F1': 0.7328698605983405, 'macro-F1': 0.4832573438149853, 'zero-one-loss': 0.94328}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Interprétations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des métriques\n",
    "- Micro-F1 : \n",
    "    - Moyenne pondérée des F1-scores, calculée sur l'ensemble des échantillons et des labels. Elle accorde plus de poids aux classes fréquentes.\n",
    "    - Indique la performance globale, favorisant les labels fréquents.\n",
    "\n",
    "- Macro-F1 : \n",
    "    - Moyenne des F1-scores par classe, non pondérée par la fréquence des classes.\n",
    "    - Évalue l'équilibre des performances sur toutes les classes, même les rares.\n",
    "\n",
    "- Zero-One-Loss : \n",
    "    - Proportion d'échantillons mal prédits entièrement (aucun label correct pour ces échantillons).\n",
    "    - Plus cette valeur est basse, plus la méthode est précise dans ses prédictions globales.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "MultiOutputClassifier Metrics :\n",
    "- Sans extraction de concept :\n",
    "    - micro-F1: 0.8282540933682061\n",
    "    - macro-F1: 0.6836620901639565\n",
    "    - zero-one-loss: 0.86224\n",
    "- Avec extraction de concept :\n",
    "    - micro-F1: 0.7492527939410732\n",
    "    - macro-F1: 0.44628391519437743\n",
    "    - zero-one-loss: 0.93528\n",
    "\n",
    "ClassifierChain Metrics:\n",
    "- Sans extraction de concept :\n",
    "    - micro-F1: 0.7800861548030066\n",
    "    - macro-F1: 0.6245708592773794\n",
    "    - zero-one-loss: 0.91156\n",
    "- Avec extraction de concept :\n",
    "    - micro-F1: 0.7323635473724367\n",
    "    - macro-F1: 0.48255564574963455\n",
    "    - zero-one-loss: 0.94308\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Micro-F1 et Macro-F1 diminuent :\n",
    "\n",
    "- La réduction de dimensions via TruncatedSVD réduit le détail et la richesse de l’information disponible pour la classification. Cela peut entraîner une perte d’efficacité, notamment pour des labels rares ou spécifiques.\n",
    "\n",
    "- Micro-F1 :\n",
    "    - Une diminution de la Micro-F1 signifie que les modèles deviennent moins performants sur les classes fréquentes. Cela peut arriver parce que :\n",
    "        - La réduction de dimensions élimine des informations discriminantes importantes pour séparer correctement les classes.\n",
    "        - Les données simplifiées via SVD ne contiennent plus suffisamment de diversité pour capturer les relations spécifiques aux labels fréquents.\n",
    "- Macro-F1 :\n",
    "    - La baisse de la Macro-F1 signifie que les performances se détériorent également sur les classes rares. Cela peut indiquer que :\n",
    "        - Les classes peu fréquentes perdent encore plus d’information lors de la réduction de dimensions.\n",
    "        - Les modèles ont du mal à distinguer des concepts fins ou spécifiques à cause de l’agrégation des données (SVD regroupe plusieurs mots synonymes ou termes proches dans des \"concepts\")\n",
    "\n",
    "Zero-One-Loss augmente :\n",
    "\n",
    "- Une augmentation de la zero-one-loss signifie que davantage d’échantillons n’ont aucun label correctement prédit. Cela peut s'expliquer par :\n",
    "    - La simplification excessive des données via SVD, qui rend plus difficile pour les modèles de capturer des associations précises entre les textes et leurs labels.\n",
    "    - Les concepts générés par SVD peuvent introduire de la confusion, en regroupant des termes ou des relations qui étaient distinctes dans les données originales.\n",
    "\n",
    "Différence entre MOC et ECC :\n",
    "\n",
    "- MultiOutputClassifier a de meilleures performances générales (Micro-F1, Macro-F1) car il considère chaque label de manière indépendante. En revanche, ClassifierChain peut souffrir si les premières étapes de la chaîne introduisent des erreurs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Impact de l'extraction de concepts\n",
    "\n",
    "- Réduction de la richesse des données :\n",
    "\n",
    "    - La représentation via SVD simplifie les données en les projetant dans un espace de dimensions réduites. Bien que cela puisse résoudre des problèmes comme la synonymie ou la polysémie, cela peut aussi éliminer des détails importants pour la classification, surtout dans des tâches complexes comme le multi-label.\n",
    "\n",
    "- Perte de granularité :\n",
    "\n",
    "    - Les \"concepts\" créés par SVD sont des abstractions qui peuvent être utiles pour des tâches générales (comme la recherche d’information ou le regroupement thématique). Cependant, pour une tâche supervisée, cette abstraction peut rendre les données moins spécifiques et donc moins informatives pour les modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Partie 2 : Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Apprentissage de notre Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = taa.train_and_save_W2V_model(X_train, 'abstractText', model_name='W2V_model.h5', model_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.bis Load W2V model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou load un model déjà calculé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = taa.load_W2V_model('W2V_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre total de mots ou tokens uniques qui ont été appris par le modèle Word2Vec\n",
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir le vecteur de mot pour un mot spécifique\n",
    "model.wv['easy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Visualisation 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.plot_word_2d(model, 'patient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Visualisation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.simple_plot_W2V_heatmap(model.wv['surgeon'])\n",
    "taa.simple_plot_W2V_heatmap(model.wv['nurse'])\n",
    "taa.simple_plot_W2V_heatmap(model.wv['boy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['surgeon', 'nurse', 'boy']\n",
    "taa.plot_W2V_heatmap(model, words=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Évaluation numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.less_similar_word(model, ['food','drink','play'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.most_similar_words(model, close_to='patient', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.most_similar_words(model, close_to=['nurse', 'doctor'], far_from='lady', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Vectorisation de notre texte (grâce à notre propre modèle W2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Vectorisation sans TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_without_tfidf_train = taa.word2vec_generator(X_train['abstractText'], model, 100)\n",
    "df_word2vec_without_tfidf_test = taa.word2vec_generator(X_test['abstractText'], model, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colonnes : Chaque colonne représente une dimension (ou feature) dans l'espace vectoriel généré par le modèle Word2Vec. Ici le modèle utilise un vector_size=100, alors on a 100 colonnes, où chaque colonne correspond à une dimension du vecteur.\n",
    "\n",
    "Lignes : Chaque ligne correspond à un document unique dans le corpus. La valeur dans chaque cellule est la moyenne des valeurs des vecteurs des mots qui composent ce document.\n",
    "\n",
    "Valeurs : Ce sont les coordonnées du vecteur moyen pour chaque document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Vectorisation avec pondération TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def word2vec_with_tfidf_optimized(texts, model, vector_size):\n",
    "    \"\"\"\n",
    "    Vectorise les textes en utilisant Word2Vec pondéré par TF-IDF en utilisant une matrice sparse.\n",
    "    \n",
    "    Parameters:\n",
    "    texts (list of str): Textes sous forme de chaînes.\n",
    "    model (Word2Vec): Modèle Word2Vec entraîné.\n",
    "    vector_size (int): Dimension des vecteurs Word2Vec.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame contenant les vecteurs moyens pondérés pour chaque texte.\n",
    "    \"\"\"\n",
    "    # Calcul des scores TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split())\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    tfidf_vocab = {word: idx for idx, word in enumerate(tfidf_vectorizer.get_feature_names_out())}\n",
    "    \n",
    "    # Construction des vecteurs pondérés\n",
    "    vectors = []\n",
    "    for i, text in enumerate(texts):\n",
    "        arr = np.zeros(vector_size)\n",
    "        valid_words = 0\n",
    "        for word in text.split():\n",
    "            if word in tfidf_vocab:  # Vérifie si le mot est dans le vocabulaire TF-IDF\n",
    "                word_index = tfidf_vocab[word]\n",
    "                tfidf_score = tfidf_matrix[i, word_index]  # Accès direct au score TF-IDF\n",
    "                try:\n",
    "                    arr += model.wv[word] * tfidf_score  # Pondère le vecteur Word2Vec\n",
    "                    valid_words += 1\n",
    "                except KeyError:\n",
    "                    continue  # Ignore les mots absents du modèle Word2Vec\n",
    "        if valid_words > 0:\n",
    "            arr /= valid_words\n",
    "        vectors.append(arr)\n",
    "    \n",
    "    return pd.DataFrame(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_tfidf_train = word2vec_with_tfidf_optimized(X_train['abstractText'], model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_tfidf_test = word2vec_with_tfidf_optimized(X_test['abstractText'], model, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Run models avec W2V sans pondérations TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.run_models(df_word2vec_without_tfidf_train, df_word2vec_without_tfidf_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running MultiOutputClassifier...\n",
    "Evaluating MultiOutputClassifier...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.75      0.77     11641\n",
    "           1       0.96      0.99      0.97     23274\n",
    "           2       0.84      0.83      0.84     13179\n",
    "           3       0.90      0.89      0.90     15486\n",
    "           4       0.81      0.96      0.88     19629\n",
    "           5       0.81      0.62      0.70      4437\n",
    "           6       0.82      0.89      0.85     16802\n",
    "           7       0.56      0.06      0.11      3048\n",
    "           8       0.67      0.45      0.54      2839\n",
    "           9       0.67      0.28      0.39      2723\n",
    "          10       0.69      0.32      0.44      3729\n",
    "          11       0.85      0.86      0.85     10623\n",
    "          12       0.80      0.76      0.78     11447\n",
    "          13       0.74      0.57      0.64      4024\n",
    "\n",
    "   micro avg       0.84      0.82      0.83    142881\n",
    "   macro avg       0.78      0.66      0.69    142881\n",
    "weighted avg       0.83      0.82      0.81    142881\n",
    " samples avg       0.85      0.83      0.82    142881\n",
    "\n",
    "\n",
    "Running ClassifierChain...\n",
    "Evaluating ClassifierChain...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.76      0.75      0.76     11641\n",
    "           1       0.96      0.98      0.97     23274\n",
    "           2       0.82      0.83      0.83     13179\n",
    "           3       0.89      0.89      0.89     15486\n",
    "           4       0.82      0.92      0.87     19629\n",
    "           5       0.76      0.65      0.70      4437\n",
    "           6       0.82      0.86      0.84     16802\n",
    "           7       0.44      0.17      0.25      3048\n",
    "           8       0.63      0.50      0.56      2839\n",
    "           9       0.59      0.35      0.44      2723\n",
    "          10       0.59      0.43      0.50      3729\n",
    "          11       0.83      0.88      0.86     10623\n",
    "          12       0.74      0.79      0.76     11447\n",
    "          13       0.67      0.60      0.63      4024\n",
    "\n",
    "   micro avg       0.82      0.82      0.82    142881\n",
    "   macro avg       0.74      0.69      0.70    142881\n",
    "weighted avg       0.81      0.82      0.81    142881\n",
    " samples avg       0.83      0.83      0.81    142881\n",
    "\n",
    "\n",
    "Results Summary:\n",
    "MultiOutputClassifier Metrics: {'micro-F1': 0.8291467836694316, 'macro-F1': 0.6894646303137411, 'zero-one-loss': 0.86792}\n",
    "ClassifierChain Metrics: {'micro-F1': 0.8205169537853408, 'macro-F1': 0.7033529020469752, 'zero-one-loss': 0.88088}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Run models avec W2V avec pondérations TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.run_models(df_word2vec_tfidf_train, df_word2vec_tfidf_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running MultiOutputClassifier...\n",
    "Evaluating MultiOutputClassifier...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.76      0.72      0.74     11641\n",
    "           1       0.95      0.99      0.97     23274\n",
    "           2       0.82      0.82      0.82     13179\n",
    "           3       0.88      0.89      0.89     15486\n",
    "           4       0.80      0.98      0.88     19629\n",
    "           5       0.80      0.53      0.64      4437\n",
    "           6       0.80      0.90      0.84     16802\n",
    "           7       0.56      0.04      0.07      3048\n",
    "           8       0.65      0.34      0.45      2839\n",
    "           9       0.65      0.18      0.29      2723\n",
    "          10       0.66      0.20      0.31      3729\n",
    "          11       0.82      0.81      0.81     10623\n",
    "          12       0.77      0.74      0.76     11447\n",
    "          13       0.72      0.44      0.55      4024\n",
    "\n",
    "   micro avg       0.83      0.80      0.81    142881\n",
    "   macro avg       0.76      0.61      0.64    142881\n",
    "weighted avg       0.81      0.80      0.79    142881\n",
    " samples avg       0.83      0.81      0.80    142881\n",
    "\n",
    "\n",
    "Running ClassifierChain...\n",
    "Evaluating ClassifierChain...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.72      0.75      0.73     11641\n",
    "           1       0.96      0.97      0.97     23274\n",
    "           2       0.80      0.80      0.80     13179\n",
    "           3       0.88      0.87      0.87     15486\n",
    "           4       0.81      0.92      0.86     19629\n",
    "           5       0.74      0.57      0.64      4437\n",
    "           6       0.81      0.85      0.83     16802\n",
    "           7       0.41      0.11      0.18      3048\n",
    "           8       0.62      0.45      0.52      2839\n",
    "           9       0.61      0.28      0.38      2723\n",
    "          10       0.56      0.33      0.42      3729\n",
    "          11       0.80      0.82      0.81     10623\n",
    "          12       0.73      0.74      0.73     11447\n",
    "          13       0.63      0.49      0.55      4024\n",
    "\n",
    "   micro avg       0.81      0.79      0.80    142881\n",
    "   macro avg       0.72      0.64      0.66    142881\n",
    "weighted avg       0.80      0.79      0.79    142881\n",
    " samples avg       0.82      0.80      0.79    142881\n",
    "\n",
    "\n",
    "Results Summary:\n",
    "MultiOutputClassifier Metrics: {'micro-F1': 0.811451493959679, 'macro-F1': 0.6428686925341808, 'zero-one-loss': 0.89272}\n",
    "ClassifierChain Metrics: {'micro-F1': 0.8005805104244098, 'macro-F1': 0.6646598089366246, 'zero-one-loss': 0.90176}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les performances globales (micro-F1 et macro-F1) sont meilleures sans pondération TF-IDF pour les deux classifieurs.\n",
    "\n",
    "La pondération TF-IDF semble introduire un bruit ou une perte d'information dans les vecteurs, ce qui diminue les scores F1 et augmente les erreurs (zero-one-loss).\n",
    "\n",
    "La pondération TF-IDF tend à dégrader les performances. Cela peut être dû à plusieurs facteurs :\n",
    "\n",
    "- Les mots très fréquents dans le corpus (et donc ayant des scores TF-IDF faibles) peuvent jouer un rôle important dans la sémantique du texte. La pondération les réduit, ce qui peut nuire à la qualité des vecteurs.\n",
    "- Le calcul de la pondération TF-IDF peut être bruyant pour des mots qui n’apparaissent pas souvent dans le corpus, ce qui affecte leur impact.\n",
    "- Les vecteurs Word2Vec étant déjà entraînés pour capturer la sémantique, pondérer par TF-IDF peut parfois altérer leurs relations structurelles.\n",
    "\n",
    "Les résultats avec TF-IDF + SVD sont significativement inférieurs à ceux de Word2Vec (sans ou avec pondération). La réduction de dimensions avec SVD entraîne une perte d'information importante, ce qui impacte les performances.\n",
    "\n",
    "Les représentations Word2Vec simples (sans pondération) surpassent nettement les représentations pondérées par TF-IDF. Cela est cohérent avec l'analyse précédente : la pondération par TF-IDF semble dégrader la richesse des vecteurs Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Modèle Word2Vec pré-entrainé de Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Charger le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Charger le fichier Word2Vec pré-entraîné\n",
    "file_path = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "\n",
    "# Vérifier si un mot existe dans le modèle\n",
    "print(\"king\" in model.key_to_index) # Affiche True si le mot \"king\" est présent dans le modèle\n",
    "\n",
    "# Accéder au vecteur d'un mot\n",
    "print(model[\"king\"])  # Affiche le vecteur associé au mot \"king\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre total de mots ou tokens uniques qui ont été appris par le modèle Word2Vec\n",
    "len(model.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "words = ['surgeon', 'nurse', 'boy']\n",
    "\n",
    "def plot_W2V_heatmap(model, words, cmap='YlGnBu', figsize_per_vector=(20, 1)):\n",
    "    \"\"\"\n",
    "    Plot a heatmap for Word2Vec vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    model (gensim.models.Word2Vec): Trained Word2Vec model.\n",
    "    words (list of str): List of words to plot.\n",
    "    cmap (str, optional): Colormap for the heatmap. Default is 'YlGnBu'.\n",
    "    figsize_per_vector (tuple, optional): Tuple specifying figure size per vector. Default is (20, 1).\n",
    "    \"\"\"\n",
    "    # Extract vectors for the given words\n",
    "    vectors = [model[word] for word in words]\n",
    "\n",
    "    # Ensure vectors are in 2D format\n",
    "    num_vectors = len(vectors)\n",
    "    fig_height = figsize_per_vector[1] * num_vectors\n",
    "    fig_width = figsize_per_vector[0]\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    sns.heatmap(vectors, cmap=cmap, cbar=True, ax=ax, xticklabels=False)\n",
    "    \n",
    "    # Set y-ticks with labels for words\n",
    "    ax.set_yticks([i + 0.5 for i in range(num_vectors)])\n",
    "    ax.set_yticklabels(words)\n",
    "    \n",
    "    ax.set_ylabel(\"Words\")\n",
    "    plt.title(\"Word2Vec Heatmap\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_W2V_heatmap(model, words=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle Word2Vec pré-entrainé de Google est bien chargé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Sans pondération TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faur refaire une fonction word2vec_generator sinon probleme de compatibilité avec les versions de Python lorsqu'on utilise le modele de Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def word2vec_generator_Google(text_column, model, vector_size=100):\n",
    "    \"\"\"\n",
    "    Génère des représentations Word2Vec moyennes pour une liste de textes.\n",
    "    \n",
    "    Parameters:\n",
    "    texts (list of list[str]): Liste de listes de mots.\n",
    "    model (gensim.models.Word2Vec): Modèle Word2Vec.\n",
    "    vector_size (int): Taille des vecteurs du modèle Word2Vec.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame où chaque ligne est la moyenne des vecteurs des mots d'un texte.\n",
    "    \"\"\"\n",
    "    # Étape 1 : Prétraitement des données (tokenisation)\n",
    "    tokenized_texts = text_column.apply(word_tokenize).tolist()\n",
    "\n",
    "    dict_word2vec = {}\n",
    "    \n",
    "    for index, word_list in enumerate(tokenized_texts):\n",
    "        arr = np.zeros(vector_size)  # Initialisation du vecteur de taille `vector_size`\n",
    "        nb_word = 0\n",
    "        \n",
    "        for word in word_list:\n",
    "            try:\n",
    "                arr += model[word]  # Accès correct au vecteur\n",
    "                nb_word += 1\n",
    "            except KeyError:\n",
    "                continue  # Ignore les mots absents du vocabulaire\n",
    "        \n",
    "        # Ajout du vecteur normalisé dans le dictionnaire\n",
    "        dict_word2vec[index] = arr / nb_word if nb_word > 0 else arr\n",
    "    \n",
    "    # Conversion du dictionnaire en DataFrame\n",
    "    df_word2vec = pd.DataFrame.from_dict(dict_word2vec, orient='index')\n",
    "    \n",
    "    print(f\"Les représentations Word2Vec moyennes ont été générées pour {len(tokenized_texts)} textes.\")\n",
    "\n",
    "    return df_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_without_tfidf_train = word2vec_generator_Google(X_train['abstractText'], model, 300)\n",
    "df_word2vec_without_tfidf_test = word2vec_generator_Google(X_test['abstractText'], model, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Avec pondération TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def word2vec_with_tfidf_optimized(texts, model, vector_size):\n",
    "    \"\"\"\n",
    "    Vectorise les textes en utilisant Word2Vec pondéré par TF-IDF en utilisant une matrice sparse.\n",
    "    \n",
    "    Parameters:\n",
    "    texts (list of str): Textes sous forme de chaînes.\n",
    "    model (Word2Vec): Modèle Word2Vec entraîné.\n",
    "    vector_size (int): Dimension des vecteurs Word2Vec.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame contenant les vecteurs moyens pondérés pour chaque texte.\n",
    "    \"\"\"\n",
    "    # Calcul des scores TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split())\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    tfidf_vocab = {word: idx for idx, word in enumerate(tfidf_vectorizer.get_feature_names_out())}\n",
    "    \n",
    "    # Construction des vecteurs pondérés\n",
    "    vectors = []\n",
    "    for i, text in enumerate(texts):\n",
    "        arr = np.zeros(vector_size)\n",
    "        valid_words = 0\n",
    "        for word in text.split():\n",
    "            if word in tfidf_vocab:  # Vérifie si le mot est dans le vocabulaire TF-IDF\n",
    "                word_index = tfidf_vocab[word]\n",
    "                tfidf_score = tfidf_matrix[i, word_index]  # Accès direct au score TF-IDF\n",
    "                try:\n",
    "                    arr += model[word] * tfidf_score  # Pondère le vecteur Word2Vec\n",
    "                    valid_words += 1\n",
    "                except KeyError:\n",
    "                    continue  # Ignore les mots absents du modèle Word2Vec\n",
    "        if valid_words > 0:\n",
    "            arr /= valid_words\n",
    "        vectors.append(arr)\n",
    "    \n",
    "    return pd.DataFrame(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_tfidf_train = word2vec_with_tfidf_optimized(X_train['abstractText'], model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec_tfidf_test = word2vec_with_tfidf_optimized(X_test['abstractText'], model, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Run pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.run_models(df_word2vec_without_tfidf_train, df_word2vec_without_tfidf_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running MultiOutputClassifier...\n",
    "\n",
    "Evaluating MultiOutputClassifier...\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.74      0.76     11641\n",
    "           1       0.95      0.99      0.97     23274\n",
    "           2       0.85      0.83      0.84     13179\n",
    "           3       0.91      0.90      0.90     15486\n",
    "           4       0.81      0.96      0.88     19629\n",
    "           5       0.82      0.62      0.71      4437\n",
    "           6       0.82      0.89      0.85     16802\n",
    "           7       0.62      0.07      0.13      3048\n",
    "           8       0.71      0.45      0.55      2839\n",
    "           9       0.71      0.28      0.40      2723\n",
    "          10       0.70      0.31      0.43      3729\n",
    "          11       0.85      0.86      0.86     10623\n",
    "          12       0.79      0.77      0.78     11447\n",
    "          13       0.76      0.57      0.65      4024\n",
    "\n",
    "   micro avg       0.85      0.82      0.83    142881\n",
    "   macro avg       0.79      0.66      0.69    142881\n",
    "weighted avg       0.84      0.82      0.82    142881\n",
    " samples avg       0.85      0.83      0.82    142881\n",
    "\n",
    "\n",
    "Running ClassifierChain...\n",
    "\n",
    "Evaluating ClassifierChain...\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.74      0.76      0.75     11641\n",
    "           1       0.96      0.98      0.97     23274\n",
    "           2       0.81      0.85      0.83     13179\n",
    "           3       0.88      0.90      0.89     15486\n",
    "           4       0.81      0.94      0.87     19629\n",
    "           5       0.76      0.60      0.67      4437\n",
    "           6       0.81      0.87      0.84     16802\n",
    "           7       0.43      0.15      0.22      3048\n",
    "           8       0.66      0.49      0.56      2839\n",
    "           9       0.64      0.31      0.42      2723\n",
    "          10       0.57      0.40      0.47      3729\n",
    "          11       0.82      0.88      0.85     10623\n",
    "          12       0.73      0.80      0.76     11447\n",
    "          13       0.65      0.55      0.60      4024\n",
    "          \n",
    "   micro avg       0.82      0.82      0.82    142881\n",
    "   macro avg       0.73      0.68      0.69    142881\n",
    "weighted avg       0.80      0.82      0.81    142881\n",
    " samples avg       0.82      0.83      0.81    142881\n",
    "\n",
    "\n",
    "Results Summary:\n",
    "\n",
    "MultiOutputClassifier Metrics: {'micro-F1': 0.8313752862286197, 'macro-F1': 0.6940577266239742, 'zero-one-loss': 0.86632}\n",
    "\n",
    "ClassifierChain Metrics: {'micro-F1': 0.8187333814248327, 'macro-F1': 0.6934683537438865, 'zero-one-loss': 0.88588}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.run_models(df_word2vec_tfidf_train, df_word2vec_tfidf_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running MultiOutputClassifier...\n",
    "Evaluating MultiOutputClassifier...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.75      0.68      0.71     11641\n",
    "           1       0.93      1.00      0.96     23274\n",
    "           2       0.82      0.80      0.81     13179\n",
    "           3       0.86      0.91      0.88     15486\n",
    "           4       0.79      0.99      0.88     19629\n",
    "           5       0.84      0.34      0.48      4437\n",
    "           6       0.76      0.92      0.83     16802\n",
    "           7       0.74      0.00      0.01      3048\n",
    "           8       0.72      0.17      0.28      2839\n",
    "           9       0.66      0.04      0.08      2723\n",
    "          10       0.81      0.03      0.05      3729\n",
    "          11       0.81      0.77      0.79     10623\n",
    "          12       0.75      0.72      0.74     11447\n",
    "          13       0.74      0.21      0.32      4024\n",
    "\n",
    "   micro avg       0.82      0.77      0.79    142881\n",
    "   macro avg       0.78      0.54      0.56    142881\n",
    "weighted avg       0.81      0.77      0.75    142881\n",
    " samples avg       0.82      0.78      0.78    142881\n",
    "\n",
    "\n",
    "Running ClassifierChain...\n",
    "Evaluating ClassifierChain...\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.68      0.75      0.71     11641\n",
    "           1       0.95      0.98      0.97     23274\n",
    "           2       0.78      0.81      0.79     13179\n",
    "           3       0.84      0.90      0.87     15486\n",
    "           4       0.81      0.93      0.86     19629\n",
    "           5       0.76      0.46      0.57      4437\n",
    "           6       0.79      0.86      0.82     16802\n",
    "           7       0.42      0.10      0.17      3048\n",
    "           8       0.66      0.37      0.47      2839\n",
    "           9       0.64      0.25      0.36      2723\n",
    "          10       0.53      0.31      0.39      3729\n",
    "          11       0.78      0.81      0.80     10623\n",
    "          12       0.72      0.70      0.71     11447\n",
    "          13       0.64      0.39      0.48      4024\n",
    "\n",
    "   micro avg       0.80      0.78      0.79    142881\n",
    "   macro avg       0.71      0.61      0.64    142881\n",
    "weighted avg       0.78      0.78      0.78    142881\n",
    " samples avg       0.80      0.79      0.78    142881\n",
    "\n",
    "\n",
    "Results Summary:\n",
    "MultiOutputClassifier Metrics: {'micro-F1': 0.7936125013502322, 'macro-F1': 0.5591234572015534, 'zero-one-loss': 0.91276}\n",
    "ClassifierChain Metrics: {'micro-F1': 0.7906897951260831, 'macro-F1': 0.6407855289731985, 'zero-one-loss': 0.91016}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle de Google ne semble pas faire mieux que notre propre modèle Word2Vec entrainé sur notre corpus de texte."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taa_tp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
