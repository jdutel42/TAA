{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : Apprentissage multi-label sur des données textuelles\n",
    "\n",
    "*Jordan Dutel et Ariane Paradan*\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import function_TAA as taa\n",
    "importlib.reload(taa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = taa.load_data('PubMed-multi-label-dataset.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taa.get_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction est longue à faire tourner (~4 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = taa.clean_text(df, 'abstractText')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Features and labels split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']\n",
    "X, y = taa.split_feature_label(df, target_column, multilabel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = taa.split_train_test(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec, X_test_vec = taa.tfidf_vectorize(X_train, X_test, max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, zero_one_loss, classification_report\n",
    "\n",
    "def runMOC(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fonction pour exécuter un MultiOutputClassifier avec une régression logistique.\n",
    "    \"\"\"\n",
    "    model = MultiOutputClassifier(LogisticRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def runECC(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fonction pour exécuter un ClassifierChain avec un k-plus proche voisin.\n",
    "    \"\"\"\n",
    "    base_model = KNeighborsClassifier(n_neighbors=5)  # Paramètre ajustable\n",
    "    chains = [ClassifierChain(base_model, order='random', random_state=i) for i in range(3)]  # Ex. 3 chaînes\n",
    "    \n",
    "    # Moyenne des prédictions des chaînes\n",
    "    preds = []\n",
    "    for chain in chains:\n",
    "        chain.fit(X_train, y_train)\n",
    "        preds.append(chain.predict(X_test))\n",
    "    \n",
    "    # Moyenne des résultats pour obtenir un seul tableau de prédictions\n",
    "    y_pred = sum(preds) / len(preds)\n",
    "    y_pred = (y_pred > 0.5).astype(int)  # Seuil pour binariser les prédictions\n",
    "    return y_pred\n",
    "\n",
    "def evaluate(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Fonction pour évaluer les performances des prédictions.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"micro-F1\": f1_score(y_test, y_pred, average='micro'),\n",
    "        \"macro-F1\": f1_score(y_test, y_pred, average='macro'),\n",
    "        \"zero-one-loss\": zero_one_loss(y_test, y_pred)\n",
    "    }\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return metrics\n",
    "\n",
    "def run_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fonction principale pour exécuter et évaluer les modèles.\n",
    "    \"\"\"\n",
    "    print(\"Running MultiOutputClassifier...\")\n",
    "    moc_pred = runMOC(X_train, X_test, y_train, y_test)\n",
    "    print(\"Evaluating MultiOutputClassifier...\")\n",
    "    moc_metrics = evaluate(y_test, moc_pred)\n",
    "    \n",
    "    print(\"\\nRunning ClassifierChain...\")\n",
    "    ecc_pred = runECC(X_train, X_test, y_train, y_test)\n",
    "    print(\"Evaluating ClassifierChain...\")\n",
    "    ecc_metrics = evaluate(y_test, ecc_pred)\n",
    "    \n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(\"MultiOutputClassifier Metrics:\", moc_metrics)\n",
    "    print(\"ClassifierChain Metrics:\", ecc_metrics)\n",
    "    return {\"MOC\": moc_metrics, \"ECC\": ecc_metrics}\n",
    "\n",
    "# Supposons que X_train, X_test, y_train, y_test soient déjà définis\n",
    "results = run_models(X_train_vec, X_test_vec, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que X_train, X_test, y_train, y_test soient déjà définis\n",
    "results = run_models(X_train_vec, X_test_vec, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = LogisticRegression()\n",
    "\n",
    "# Run EnsembleClassifierChain\n",
    "def runMOC(X_train, y_train, X_test):\n",
    "    clf = MultiOutputClassifier(LogisticRegression()).fit(X_train_tfidf, y_train)\n",
    "    clf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "def runECC(X_train, y_train, X_test, model_name):\n",
    "    chains = [ClassifierChain(base_lr, order=\"random\", random_state=i) for i in range(10)]\n",
    "    for chain in chains:\n",
    "        chain.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred_chains = np.array([chain.predict_proba(X_test) for chain in chains])\n",
    "    chain_jaccard_scores = [\n",
    "        jaccard_score(Y_test, Y_pred_chain >= 0.5, average=\"samples\")\n",
    "        for Y_pred_chain in Y_pred_chains\n",
    "    ]\n",
    "\n",
    "    Y_pred_ensemble = Y_pred_chains.mean(axis=0)\n",
    "    ensemble_jaccard_score = jaccard_score(\n",
    "        Y_test, Y_pred_ensemble >= 0.5, average=\"samples\"\n",
    "    )\n",
    "    return ensemble_jaccard_score\n",
    "\n",
    "def Evaluate(PRED, y_test):\n",
    "\n",
    "    return \n",
    "\n",
    "def run_modele(X_train, y_train, X_test, y_test):\n",
    "    PRED_MOC_LR = runMOC(X_train, y_train, X_test)\n",
    "    PRED_ECC_LR = runECC(X_train, y_train, X_test)\n",
    "    Evaluate(PRED_MOC_LR, y_test)  \n",
    "    Evaluate(PRED_ECC_LR, y_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import jaccard_score, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def runMOC(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    MultiOutputClassifier avec régression logistique\n",
    "    \"\"\"\n",
    "    clf = MultiOutputClassifier(LogisticRegression(max_iter=2000, random_state=42))\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def runECC(X_train, y_train, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Ensemble Classifier Chain avec 10 chaînes\n",
    "    \"\"\"\n",
    "    base_lr = LogisticRegression(max_iter=2000, random_state=42)\n",
    "    chains = [ClassifierChain(base_lr, order=\"random\", random_state=i) for i in range(10)]\n",
    "\n",
    "    # Ajustement des chaînes\n",
    "    for chain in chains:\n",
    "        chain.fit(X_train, y_train)\n",
    "\n",
    "    # Prédictions pour chaque chaîne\n",
    "    Y_pred_chains = np.array([chain.predict(X_test) for chain in chains])\n",
    "    Y_pred_ensemble = Y_pred_chains.mean(axis=0) >= 0.5  # Moyenne des prédictions\n",
    "    \n",
    "    # Évaluation des performances de l'ensemble\n",
    "    chain_jaccard_scores = [\n",
    "        jaccard_score(Y_test, Y_pred_chain, average=\"samples\") for Y_pred_chain in Y_pred_chains\n",
    "    ]\n",
    "    ensemble_jaccard_score = jaccard_score(\n",
    "        Y_test, Y_pred_ensemble, average=\"samples\"\n",
    "    )\n",
    "\n",
    "    print(f\"Scores Jaccard des chaînes individuelles : {chain_jaccard_scores}\")\n",
    "    print(f\"Score Jaccard de l'ensemble : {ensemble_jaccard_score}\")\n",
    "\n",
    "    return Y_pred_ensemble\n",
    "\n",
    "\n",
    "def evaluate(PRED, y_test):\n",
    "    \"\"\"\n",
    "    Évaluation des performances du modèle\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_test, PRED)\n",
    "    report = classification_report(y_test, PRED, zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy : {accuracy}\")\n",
    "    print(\"Rapport de classification :\")\n",
    "    print(report)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def run_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Exécution des modèles et évaluation des performances\n",
    "    \"\"\"\n",
    "    print(\"\\n=== MultiOutputClassifier ===\")\n",
    "    PRED_MOC_LR = runMOC(X_train, y_train, X_test)\n",
    "    evaluate(PRED_MOC_LR, y_test)\n",
    "\n",
    "    print(\"\\n=== Ensemble Classifier Chains ===\")\n",
    "    PRED_ECC_LR = runECC(X_train, y_train, X_test, y_test)\n",
    "    evaluate(PRED_ECC_LR, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_train_tfidf, y_train, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_tfidf shape:\", X_train_tfidf.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taa_tp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
